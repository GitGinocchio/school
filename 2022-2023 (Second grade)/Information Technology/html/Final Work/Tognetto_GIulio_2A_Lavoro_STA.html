<!DOCTYPE html>
<html lang= "IT-it">


<head>
    <link rel="shortcut icon" href="static/images/MachineLearning_icon.png" type="image/x-icon">
    <link href="static/style.css" rel="stylesheet">
    <title> Cos'è il Machine Learning? </title>

    <! SOURCES: >
    <! https://www.ibm.com/it-it/cloud/learn/machine-learning?utm_content=SRCWW&p1=Search&p4=43700068096860752&p5=e&gclid=Cj0KCQiAsoycBhC6ARIsAPPbeLt_x_QCEDoPFjI91gFqw7y0y0-pX98tmJkAKJ2TlLD7sAssffOW0UIaAkj6EALw_wcB&gclsrc=aw.ds#toc-cos-il-mac-kR38wSIA >
    <! https://www.oracle.com/it/artificial-intelligence/machine-learning/what-is-machine-learning/ >
</head>

<body>
    <div class="top">
        <br>
        <h1 class="bigtitle"> Machine Learning</h1>

        <p style="text-align: center;"> Questa introduzione al machine learning fornisce una panoramica della sua storia, importanti definizioni, e applicazioni. </p>
        <br>
        <br>
        <hr/>
    </div>
    <br>
    <font face="Calibri" color="black"> 
        <font face="Helvetica" color="black"><h2 class="title"> Cos'è il machine learning? </h2></font>

        <p> Il machine learning è un ramo dell'<a href="https://www.ibm.com/it-it/cloud/learn/what-is-artificial-intelligence"> AI (artificial intelligence) </a> e della computer science che si concentra sull'utilizzo di dati e algoritmi per imitare il modo in cui gli esseri umani apprendono, migliorando in modo graduale la sua accuratezza.</p>
        <p> il machine learning ha una vasta <a href="https://www.ibm.com/ibm/history/ibm100/us/en/icons/ibm700series/impacts/">storia</a>. La coniazione del termine "machine learning" è attribuita a uno dei suoi, Arthur Samuel, con la sua <a href="https://hci.iwr.uni-heidelberg.de/system/files/private/downloads/636026949/report_frank_gabel.pdf">ricerca </a> (PDF, 481 KB) (link esterno) sul gioco della dama. Robert Nealey, l'autoproclamato maestro di dama, si cimentò in una partita a dama su un computer IBM 7094 nel 1962 e fu battuto dal computer. Rispetto a quello che si può fare oggi, questa impresa sembra quasi banale, ma è considerata un'importante pietra miliare nel campo dell'AI. Nei decenni successivi, gli sviluppi tecnologici che interesseranno lo storage e la potenza di elaborazione renderanno possibili alcuni innovativi prodotti che noi oggi conosciamo e apprezziamo, come il motore di consigli di Netflix o le auto a guida autonoma.</p>
        <p>Il machine learning è una componente importante del crescente campo della data science. Mediante l'uso di metodi statistici, gli algoritmi vengono addestrati per effettuare classifiche o previsioni, scoprendo degli insight chiave all'interno dei progetti di data mining. Questi insight, successivamente, promuovono il processo decisionale nelle applicazioni e nelle aziende, incidendo auspicabilmente sulle metriche di crescita. Poiché i big data continuano ad espandersi e a crescere, i data scientist saranno sempre più richiesti sul mercato, e verrà loro richiesto di assistere nell'identificazione delle problematiche di business più pertinenti e, successivamente, dei dati che forniscano loro una risposta.</p>
        
        <br>
        <hr/>
        <br>

        <font face="Helvetica" color="black"><h2 class="title"> Confronto tra machine learning, deep learning e reti neurali </h2></font>

        <p> Dal momento che il deep learning e il machine learning tendono ad essere utilizzati in modo intercambiabile, vale la pena notare le sfumature che li distinguono. Il machine learning, il deep learning e le reti neurali sono tutti sottocampi dell'AI. Tuttavia, il deep learning è in effetti un sottocampo del machine learning, mentre le reti neurali sono un sottocampo del deep learning.</p>
    
        <p> La discriminante tra deep learning e machine learning è la modalità di apprendimento di ciascun algoritmo. Il deep learning automatizza gran parte del processo relativo all'estrazione di caratteristiche, eliminando parte dell'intervento umano e consentendo l'uso di dataset più grandi. Puoi pensare al deep learning come a un "machine learning scalabile", come osserva Lex Fridman in <a href="https://youtu.be/O5xeyoRL95U?t=29">questa lezione al MIT (00:30) </a>(link esterno). Il machine learning classico, o "non profondo", dipende in misura maggiore dall'intervento umano per apprendere. Gli esperti umani determinano il set di caratteristiche per comprendere le differenze tra gli input di dati, di solito richiedendo dati più strutturati per l'apprendimento.</p>
    
        <p> Il machine learning "profondo" può sfruttare i dataset etichettati, cosa nota anche come apprendimento supervisionato, per informare il suo algoritmo, ma non richiede necessariamente un dataset etichettato. Può inserire i dati non strutturati nel loro formato non elaborato (ad es. testo, immagini) e può determinare automaticamente il set di caratteristiche che distingue tra loro le diverse categorie di dati. A differenza del machine learning, non richiede l'intervento umano per elaborare i dati, consentendoci di ridimensionare il machine learning in modi più interessanti. Al deep learning e alle reti neurali viene riconosciuto in particolare il merito di avere accelerato gli avanzamenti in aree quali la visione artificiale, NLP (natural language processing) e il riconoscimento vocale.</p>
    

        <p> Le reti neurali, o reti neurali artificiali (ANN), sono composte da livelli di nodi che contengono un livello di input, uno o più livelli nascosti e un livello di output. Ciascun nodo, o neurone artificiale, si connette ad un altro e ha un peso e una soglia associati. Se l'output di qualsiasi singolo nodo è al di sopra del valore di soglia specificato, tale nodo viene attivato, inviando i dati al successivo livello della rete. In caso contrario, non viene passato alcun dato al livello successivo della rete. Il termine "deep" (ossia "profondo") in deep learning si riferisce semplicemente alla profondità dei livelli in una rete neurale. Una rete neurale che consiste in più di tre livelli - che sarebbero comprensivi degli input e dell'output - può essere considerata un algoritmo di deep learning o una rete neurale profonda. Una rete neurale che ha solo due o tre livelli è soltanto una rete neurale di base.</p>
        <img src="https://www.eage.it/data/eage/neural-network.gif" height="300" width="450">

        <br>
        <br>
        <hr/>
        <br>

        <font face="Helvetica" color="black"> <h2 class="title"> Come funziona il machine learning</h2></font>

        <p><a href="https://ischoolonline.berkeley.edu/blog/what-is-machine-learning/">L'Università della California Berkeley </a> (link esterno) suddivide il sistema di apprendimento di un algoritmo di machine learning in tre parti principali. </p>
        
        <ol>
            <li><b>Un processo decisionale: </b> in generale, gli algoritmi di machine learning sono utilizzati per fare una previsione o una classificazione. In base ad alcuni dati di input, che possono essere etichettati o non etichettati, il tuo algoritmo produrrà una stima su un pattern nei dati.</li>
            <br>
            <li><b>Una funzione di errore:</b> una funzione di errore serve a valutare la previsione del modello. Se esistono degli esempi noti, una funzione di errore può fare un confronto per valutare l'accuratezza del modello.</li>
            <br>
            <li><b>Un processo di ottimizzazione del modello: </b> se il modello può adattarsi meglio ai punti di dati nel set di addestramento, allora i pesi vengono regolati per ridurre la discrepanza tra l'esempio conosciuto e la stima del modello. L'algoritmo ripeterà questo processo di valutazione e ottimizzazione, aggiornando i pesi in modo autonomo fino a raggiungere una soglia di accuratezza.</li>
        </ol>

        <br>
        <hr/>
        <br>

        <font face="Helvetica" color="black"><h2 class="title"> Metodi di machine learning</h2></font>
        <p>I classificatori di machine learning rientrano in tre categorie principali.</p>

        <font face="Helvetica" color="black"><h4 class="subtitle">Machine learning supervisionato</h4></font>
        <p> <a href="https://www.ibm.com/cloud/learn/supervised-learning">L'apprendimento supervisionato</a>, noto anche come machine learning supervisionato, è definito dal suo utilizzo di dataset etichettati per addestrare gli algoritmi per classificare i dati o prevedere i risultati in modo accurato. Man mano che i dati vengono alimentati nel modello, ne regola i pesi fino a quando il fitting del modello non sarà appropriato. Questo avviene come parte del processo di convalida incrociata per garantire che il modello eviti l'<a href="https://www.ibm.com/cloud/learn/overfitting">overfitting</a> o l'<a href="https://www.ibm.com/cloud/learn/underfitting">underfitting</a>. L'apprendimento supervisionato aiuta le organizzazioni a risolvere una varietà di problemi del mondo reale su vasta scala, come la classificazione della posta indesiderata in una cartella separata rispetto alla tua posta in arrivo. Alcuni metodi utilizzati nell'apprendimento supervisionato includono, tra gli altri, le reti neurali, i classificatori bayesiani, la regressione lineare, la regressione logistica, la foresta casuale e SVM (support vector machine).</p>

        <font face="Helvetica" color="black"><h4 class="subtitle"> Machine learning non supervisionato</h4></font>
        <p> <a href="https://www.ibm.com/cloud/learn/unsupervised-learning">L'apprendimento non supervisionato</a>, noto anche come machine learning non supervisionato, utilizza gli algoritmi di machine learning per analizzare e organizzare in cluster i dataset senza etichette. Questi algoritmi scoprono i raggruppamenti di dati o i pattern nascosti senza la necessità di un intervento umano. La sua capacità di scoprire similitudini e differenze nelle informazioni la rendono la soluzione ideale per l'analisi dei dati esplorativi, le strategie di cross-selling, la segmentazione dei clienti e il riconoscimento di immagini e pattern. Si usa anche per ridurre il numero di caratteristiche in un modello attraverso il processo di riduzione della dimensionalità; PCA (principal component analysis) e SVD (singular value decomposition) sono due approcci comuni per questa attività. Altri algoritmi utilizzati nell'apprendimento non supervisionato includono, tra l'altro, le reti neurali, il clustering K-means e i metodi di clustering probabilistico.</p>

        <font face="Helvetica" color="black"><h4 class="subtitle"> Apprendimento parzialmente supervisionato</h4></font>
        <p> L'apprendimento parzialmente supervisionato rappresenta il giusto compromesso tra apprendimento supervisionato e non supervisionato. Durante l'addestramento, utilizza un dataset etichettato più piccolo per guidare la classificazione e l'estrazione di caratteristiche da un dataset non etichettato di dimensioni maggiori. L'apprendimento parzialmente supervisionato può risolvere il problema di non disporre di un numero sufficiente di dati (o di non potersi permettere di etichettare un numero sufficiente di dati) per formare un algoritmo di apprendimento con supervisione.</p>


        <br>
        <hr/>
        <br>

        <font face="Helvetica" color="black"><h2 class="title"> Machine learning per rinforzo</h2></font>
        <p> Il machine learning per rinforzo è un modello di machine learning comportamentale simile all'apprendimento supervisionato ma l'algoritmo non viene addestrato utilizzando dati di esempio. Questo modello apprende le informazioni durante l'utilizzo, tramite prove ed errori. Una sequenza di risultati positivi verrà rinforzata per sviluppare il miglior suggerimento o politica per un determinato problema. Il sistema IBM Watson® che ha vinto la sfida Jeopardy! nel 2011 ne costituisce un ottimo esempio. Il sistema <a href="https://developer.ibm.com/articles/cc-reinforcement-learning-train-software-agent/">ha utilizzato l'apprendimento per rinforzo</a> per decidere se tentare una risposta (o domanda, a seconda dei casi), quale riquadro selezionare e quanto scommettere, specialmente sui raddoppi giornalieri. </p>
        <br>

        <p> Ecco un esempio di Machine learning per rinforzo, applicato sul gioco del nascondino</p>

        <br>

        <! Default Width 560>
        <! default Height 315>
        <iframe width="746" height="420" src="https://www.youtube.com/embed/Lu56xVlZ40M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        
        <br>
        <br>
        <br>
        <hr/>
        <br>

        <font face="Helvetica" color="black"><h2 class="title"> Casi di utilizzo del machine learning nel mondo reale</h2></font>
        <p>Ecco solo alcuni esempi di machine learning che potresti incontrare ogni giorno:</p>
        
        <p><b>Riconoscimento vocale:</b> è noto anche come riconoscimento vocale automatico (ASR, automatic speech recognition), riconoscimento vocale del computer o conversione del parlato in testo scritto (STT, Speech-to-text) ed è una funzionalità che utilizza NLP (natural language processing) per elaborare il parlato umano in un formato scritto. Molti dispositivi mobili incorporano il riconoscimento vocale nei loro sistemi per effettuare ricerche vocali, ad es. Siri, o per fornire una maggiore accessibilità per quanto riguarda la scrittura di messaggi.</p>

        <p><b>Servizio clienti:</b> i chatbot online stanno sostituendo gli agenti umani lungo il percorso del cliente. Rispondono a domande frequenti sugli argomenti, come la spedizione, o forniscono consigli personalizzati, eseguendo il cross-selling di prodotti o consigliando le taglie per gli utenti, cambiando il modo in cui pensiamo al coinvolgimento del cliente nei siti web e sulle piattaforme di social media. Degli esempi includono i bot di messaggistica sui siti di e-commerce con gli agenti virtuali, le app di messaggistica, come Slack e Facebook Messenger, e le attività solitamente effettuate da assistenti virtuali e assistenti vocali.</p>

        <p><b>Visione artificiale:</b> questa tecnologia di AI consente ai computer e ai sistemi di ricavare informazioni significative da immagini digitali, video o altri input visivi e, sulla base di questi input, può intervenire. Questa capacità di fornire consigli la distingue dalle attività di riconoscimento delle immagini. Basato sulle reti neurali convolutive (CNN, convolutional neural network), la visione artificiale del computer ha applicazioni nel campo dell'aggiunta di tag a foto nei social media, nell'imaging radiologico nell'assistenza sanitaria e nelle auto a guida autonoma nell'industria automobilistica.</p>

        <p><b>Motori di consigli:</b> utilizzando i dati relativi ai comportamenti di consumo passati, gli algoritmi di AI possono aiutare a scoprire le tendenze dei dati che possono essere utilizzate per sviluppare strategie di cross-selling più efficaci. Se ne fa uso per offrire dei consigli di prodotti aggiuntivi pertinenti ai clienti durante il processo di pagamento per i rivenditori online.</p>

        <p><b>Trading azionario automatizzato:</b> progettate per ottimizzare i portafogli azionari, le piattaforme di trading ad alta frequenza basate sull'AI effettuano migliaia o anche milioni di operazioni di trading al giorno senza intervento umano.</p>
    
    
    </font>
    <br>
    <br>

</body>

<footer>
    <hr/>
    <br>
    <h3> Sources: </h3>
    <h4> <a href="https://www.ibm.com/it-it/cloud/learn/machine-learning?utm_content=SRCWW&p1=Search&p4=43700068096860752&p5=e&gclid=Cj0KCQiAsoycBhC6ARIsAPPbeLt_x_QCEDoPFjI91gFqw7y0y0-pX98tmJkAKJ2TlLD7sAssffOW0UIaAkj6EALw_wcB&gclsrc=aw.ds#toc-cos-il-mac-kR38wSIA"> IBM Cloud Education</a></h4>
    <br>
    <h3> Videos: </h3>
    <h4> <a href="https://youtu.be/Lu56xVlZ40M"> Two Minutes Papers: OpenAI Plays Hide and Seek…and Breaks The Game! 🤖</a></h4>
    <br>
    <br>
</footer>




</html>